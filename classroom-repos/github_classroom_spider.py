import os
import json
import requests
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timedelta, timezone
from collections import Counter, defaultdict
import re
import math
import pytz
import dotenv
import git

CONFIG = {
    "semestar_range": {
        "25spring": (
            datetime(2025, 2, 1, tzinfo=timezone(timedelta(hours=8))),
            datetime(2025, 6, 30, tzinfo=timezone(timedelta(hours=8))),
        ),
        "24spring": (
            datetime(2024, 2, 1, tzinfo=timezone(timedelta(hours=8))),
            datetime(2024, 6, 30, tzinfo=timezone(timedelta(hours=8))),
        ),
        "23spring": (
            datetime(2023, 2, 1, tzinfo=timezone(timedelta(hours=8))),
            datetime(2023, 6, 30, tzinfo=timezone(timedelta(hours=8))),
        ),
    },
    "classroom_id": {
        "25spring": "253851",
        "24spring": "206118",
        "23spring": "152718",
    },
    "assignment_id": {
        "23spring": "403123",
        "24spring": "558214",
        "25spring": "749620",
    },
    "valid_extensions": [
        ".py", ".java", ".js", ".jsx", ".ts", ".tsx", ".c", ".cpp", ".h", ".html", ".css", ".md",
    ],
    "single_file_insertion_limit": 2000,
}


# ========== GithubClassroomSpider ===========
class GithubClassroomSpider:
    """
    GithubClassroomSpider å°è£…äº†æ‰€æœ‰ä¸Github Classroomç›¸å…³çš„çˆ¬è™«ä¸æ•°æ®å¤„ç†åŠŸèƒ½ã€‚
    - æ‰€æœ‰ä¸´æ—¶æ•°æ®ç»Ÿä¸€å­˜å‚¨åœ¨ tmp.json
    - æ‰€æœ‰å›¾è¡¨æ•°æ®ç»Ÿä¸€å­˜å‚¨åœ¨ chart_data.json
    - è¯¦ç»†æ—¥å¿—ä¸æ³¨é‡Šï¼Œä¾¿äºç»´æŠ¤
    """

    def __init__(self, organization="sustech-cs304", repos_dir="./repos"):
        dotenv.load_dotenv()
        self.organization = organization
        self.repos_dir = repos_dir
        self.GITHUB_TOKEN = os.getenv("GITHUB_TOKEN")
        if not self.GITHUB_TOKEN:
            raise Exception("è¯·è®¾ç½®ç¯å¢ƒå˜é‡ GITHUB_TOKEN")
        self.HEADERS = {"Authorization": f"Bearer {self.GITHUB_TOKEN}"}
        self.API_URL = "https://api.github.com/graphql"
        self.tmp_path = "tmp.json"
        self.chart_data_path = "chart_data.json"
        self.tmp_data = self._load_json(self.tmp_path) or {}
        self.chart_data = self._load_json(self.chart_data_path) or {}
        self.WORKERS = 16
        self.MULTI_THREAD = True
        self.china_tz = pytz.timezone("Asia/Shanghai")
        self.semestar_range = CONFIG["semestar_range"]
        self.classroom_id = CONFIG["classroom_id"]
        self.assignment_id = CONFIG["assignment_id"]
        self.request_headers = {
            "Authorization": f"token {self.GITHUB_TOKEN}",
            "Accept": "application/vnd.github.v3+json",
        }
        self.valid_extensions = CONFIG["valid_extensions"]
        self.single_file_insertion_limit = CONFIG["single_file_insertion_limit"]
        self._log("GithubClassroomSpider åˆå§‹åŒ–å®Œæˆã€‚")

    def _log(self, msg):
        print(
            f"[GithubClassroomSpider] {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - {msg}"
        )

    def _load_json(self, path):
        if os.path.exists(path):
            with open(path, "r", encoding="utf-8") as f:
                return json.load(f)
        return None

    def _save_json(self, path, data):
        with open(path, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=2, ensure_ascii=False)

    def _run_query(self, query, variables):
        response = requests.post(
            self.API_URL,
            json={"query": query, "variables": variables},
            headers=self.HEADERS,
            timeout=15,
        )
        if response.status_code == 200:
            return response.json()
        else:
            raise Exception(
                f"GraphQL query failed: {response.status_code}\n{response.text}"
            )

    def extract_team_name_from_repo(self, semestar_name, repo_name):
        if semestar_name == "25spring":
            match_res = re.match(
                r"team-project-25spring-(team[_-]?\d+|\d+)$", repo_name
            )
            if match_res:
                return match_res.group(1)
            else:
                return None
        elif semestar_name == "24spring":
            match_res = re.match(
                r"team-project-24spring-(team[_-]?\d+|\d+)$", repo_name
            )
            if match_res:
                return match_res.group(1)
            else:
                return None
        elif semestar_name == "23spring":
            match_res = re.match(r"team-project-(\d+)$", repo_name)
            if match_res:
                return match_res.group(1)
            else:
                return None
        else:
            raise Exception(f"Invalid semestar name: {semestar_name}")

    # ç”¨äºæŸ¥è¯¢github classroomçš„classroom_idå’Œassignment_idçš„å·¥å…·
    # å½“æ–°å­¦æœŸå¼€å§‹æ—¶ï¼Œå¯ä»¥ä½¿ç”¨è¿™ä¸ªå‡½æ•°è·å–æ–°å­¦æœŸçš„classroom idå’Œassignment id
    def get_project_assignment_id(self):
        def get_classrooms():
            url = "https://api.github.com/classrooms"
            classrooms = []
            page = 1
            while True:
                resp = requests.get(
                    url,
                    headers=self.request_headers,
                    params={"page": page, "per_page": 100},
                    timeout=15,
                )
                if resp.status_code != 200:
                    self._log(
                        f"Error fetching classrooms: {resp.status_code} - {resp.text}"
                    )
                    return []
                data = resp.json()
                if not data:
                    break
                classrooms.extend(data)
                page += 1
            return classrooms

        def get_assignments(classroom_id):
            url = f"https://api.github.com/classrooms/{classroom_id}/assignments"
            assignments = []
            page = 1
            while True:
                resp = requests.get(
                    url,
                    headers=self.request_headers,
                    params={"page": page, "per_page": 100},
                    timeout=15,
                )
                if resp.status_code != 200:
                    self._log(
                        f"Error fetching assignments: {resp.status_code} - {resp.text}"
                    )
                    return []
                data = resp.json()
                if not data:
                    break
                assignments.extend(data)
                page += 1
            return assignments

        self._log("ğŸ“˜ è·å– Classrooms...")
        classrooms = get_classrooms()
        if not classrooms:
            self._log("æ²¡æœ‰æ‰¾åˆ°ä»»ä½• Classroomã€‚")
            exit(1)

        for c in classrooms:
            self._log(f"Classroom åç§°: {c['name']}")
            self._log(f"Classroom ID: {c['id']}")
            self._log("-" * 30)

        # æ‰‹åŠ¨æŒ‡å®šæˆ–é€‰æ‹©ä¸€ä¸ª Classroom
        classroom_id = input("è¯·è¾“å…¥ä½ è¦æŸ¥çœ‹çš„ Classroom ID: ").strip()

        self._log(f"\nğŸ“˜ è·å– Classroom ID ä¸º {classroom_id} çš„æ‰€æœ‰ä½œä¸š...")
        assignments = get_assignments(classroom_id)
        if not assignments:
            self._log("è¯¥ Classroom ä¸‹æ²¡æœ‰ä½œä¸šã€‚")
            exit(1)

        for a in assignments:
            self._log(f"ä½œä¸šæ ‡é¢˜: {a['title']}")
            self._log(f"Assignment ID: {a['id']}")
            self._log(f"ç±»å‹: {'å°ç»„ä½œä¸š' if a['type'] == 'group' else 'ä¸ªäººä½œä¸š'}")
            self._log("-" * 30)

    # ========== æ•°æ®å‡†å¤‡1. è·å–classroomä¸­æˆå‘˜ä¿¡æ¯ ===========
    def get_classroom_members(self):
        ORG_MEMBERS_QUERY = """
            query($org: String!, $first: Int!, $after: String) {
            organization(login: $org) {
                membersWithRole(first: $first, after: $after) {
                pageInfo {
                    hasNextPage
                    endCursor
                }
                nodes {
                    id
                    login
                    name
                    email
                }
                }
            }
            }
        """

        self._log("ğŸ” è·å–github classroomä¸­æ‰€æœ‰æˆå‘˜ä¿¡æ¯")
        all_members = []
        has_next = True
        cursor = None

        while has_next:
            variables = {"org": self.organization, "first": 100, "after": cursor}
            result = self._run_query(ORG_MEMBERS_QUERY, variables)
            data = result["data"]["organization"]["membersWithRole"]
            all_members.extend(data["nodes"])
            has_next = data["pageInfo"]["hasNextPage"]
            cursor = data["pageInfo"]["endCursor"]

        users = [m for m in all_members]
        self._log(f"âœ… è·å–æˆåŠŸï¼Œä¸€å…±æœ‰ {len(users)} åé€‰è¯¾åŒå­¦")
        self.tmp_data["classroom_members"] = users
        self._save_json(self.tmp_path, self.tmp_data)

    # ========== æ•°æ®å‡†å¤‡1. è·å–projectå°ç»„ä¸­æˆå‘˜ä¿¡æ¯ ===========
    def get_accepted_assignments(
        self, semestar_list=("23spring", "24spring", "25spring")
    ):
        self._log("å¼€å§‹è·å–å­¦æœŸprojectçš„å°ç»„æˆå‘˜ä¿¡æ¯")
        if "full_group_info" not in self.tmp_data:
            self.tmp_data["full_group_info"] = {}
        if "group_members" not in self.tmp_data:
            self.tmp_data["group_members"] = {}
        for semestar in semestar_list:
            assignment_id = self.assignment_id[semestar]
            self.tmp_data["group_members"][semestar] = {}
            self.tmp_data["full_group_info"][semestar] = []
            # Important! é»˜è®¤çš„é¡µæ•°ä¸Šé™æ˜¯30ï¼Œè€ƒè™‘åˆ°ä¸€ä¸ªå­¦æœŸä¸€èˆ¬ä¼šæœ‰40å·¦å³ä¸ªå°ç»„ï¼Œæ‰€ä»¥è¿™é‡Œè®¾ç½®ä¸º100ï¼Œä¸éœ€è¦åˆ†é¡µè·å–
            url = f"https://api.github.com/assignments/{assignment_id}/accepted_assignments?per_page=100"
            resp = requests.get(url, headers=self.request_headers, timeout=15)

            if resp.status_code != 200:
                self._log(
                    f"Error fetching accepted assignments: {resp.status_code} - {resp.text}"
                )
                continue
            data = resp.json()
            for accepted_assignment in data:
                repo_name = accepted_assignment["repository"]["name"]
                students = accepted_assignment["students"]
                self.tmp_data["group_members"][semestar][repo_name] = students

            self.tmp_data["full_group_info"][semestar].append(data)
            self._log(f"å·²è·å– {semestar} å­¦æœŸçš„å°ç»„æˆå‘˜ä¿¡æ¯ã€‚")
        self._log("å·²è·å–æ‰€æœ‰å­¦æœŸçš„å°ç»„æˆå‘˜ä¿¡æ¯ã€‚")
        self._save_json(self.tmp_path, self.tmp_data)

    def join_classroom_members_with_group_members(self):
        self._log("å¼€å§‹å°†å°ç»„æˆå‘˜ä¿¡æ¯ä¸classroomæˆå‘˜ä¿¡æ¯è¿›è¡Œå…³è”...")
        classroom_members = self.tmp_data["classroom_members"]
        login2id = {m["login"]: m["id"] for m in classroom_members}
        sucess_count, fail_count = 0, 0
        for semestar, repo_members in self.tmp_data["group_members"].items():
            for repo_name, students in repo_members.items():
                for student in students:
                    if student["login"] not in login2id:
                        self._log(f"âš ï¸ æ— æ³•æ‰¾åˆ° {student['login']} çš„github id")
                        student["github_id"] = None
                        fail_count += 1
                        continue
                    student["github_id"] = login2id[student["login"]]
                    sucess_count += 1
        self._log(
            f"å…³è”å®Œæˆã€‚æˆåŠŸå…³è” {sucess_count} ååŒå­¦ï¼Œå¤±è´¥ {fail_count} ååŒå­¦ã€‚"
        )
        self._save_json(self.tmp_path, self.tmp_data)

    # ========== 2. æœ¬åœ°ä»“åº“æ•°æ®æ”¶é›† ===========
    def gather_data_from_local_repos(self):
        """
        éå†æœ¬åœ°æ‰€æœ‰å›¢é˜Ÿä»“åº“ï¼Œæ”¶é›†commitå’Œä»£ç è¡Œæ•°ç­‰ä¿¡æ¯ï¼Œå­˜å…¥tmp_data['local_data']
        """

        def extract_semestar_name_from_path(path):
            # ä»æ–‡ä»¶å¤¹åä¸­æå–å­¦æœŸå
            match = re.search(r"team-project-(\w+)-submissions", path)
            if match:
                return f"{match.group(1)}"
            else:
                raise Exception(f"Invalid folder name: {path}")

        def check_repo_name_valid(semestar_name, repo_path):
            # æ ¡éªŒrepoæ–‡ä»¶å¤¹åæ˜¯å¦åˆæ³•ï¼Œå¹¶æå–ç»„å
            folder_name = os.path.basename(os.path.normpath(repo_path))
            return self.extract_team_name_from_repo(semestar_name, folder_name)

        def check_valid(semestar, author_name, datatime):
            # æ£€æŸ¥commitæ—¶é—´æ˜¯å¦åœ¨å­¦æœŸèŒƒå›´å†…
            if semestar not in self.semestar_range:
                return False, f"å­¦æœŸä¸å­˜åœ¨ {semestar}"
            start_date, end_date = self.semestar_range[semestar]
            if isinstance(datatime, str):
                try:
                    datatime = datetime.fromisoformat(datatime.replace("Z", "+00:00"))
                except Exception as e:
                    self._log(f"âš ï¸ æ— æ³•è§£ææ—¶é—´: {datatime}, é”™è¯¯: {e}")
                    return False, f"æ— æ³•è§£ææ—¶é—´: {datatime}, é”™è¯¯: {e}"
            time_check = start_date <= datatime <= end_date
            author_check = True  # è¿™é‡Œå¯æ ¹æ®éœ€è¦æ ¡éªŒä½œè€…
            return time_check, ""

        self._log("å¼€å§‹æ”¶é›†æœ¬åœ°ä»“åº“æ•°æ®...")
        semestar_dirs = [
            d
            for d in os.listdir(self.repos_dir)
            if os.path.isdir(os.path.join(self.repos_dir, d))
        ]
        all_results = {}
        for semestar in semestar_dirs:
            semestar_path = os.path.join(self.repos_dir, semestar)
            semestar_name = extract_semestar_name_from_path(semestar)
            repo_paths = []
            for name in os.listdir(semestar_path):
                repo_path = os.path.join(semestar_path, name)
                git_folder = os.path.join(repo_path, ".git")
                if os.path.isdir(repo_path) and os.path.isdir(git_folder):
                    repo_paths.append(repo_path)
            results = []

            def process_repo(repo_path):
                try:
                    repo = git.Repo(repo_path)
                    repo_name = os.path.basename(repo_path)
                    group_name = check_repo_name_valid(semestar_name, repo_path)
                    if group_name is None:
                        return None
                    all_commits = set()
                    commit_info_list = []
                    commit_file_stats = {ext: 0 for ext in self.valid_extensions}
                    for branch in repo.branches:
                        for commit in repo.iter_commits(branch):
                            if commit.hexsha not in all_commits:
                                all_commits.add(commit.hexsha)
                                valid, _ = check_valid(
                                    semestar_name,
                                    commit.author.name,
                                    commit.committed_datetime.isoformat(),
                                )
                                if not valid:
                                    continue
                                stats = commit.stats.total
                                commit_info = {
                                    "commit_hash": commit.hexsha,
                                    "author_name": commit.author.name,
                                    "author_email": commit.author.email,  # æäº¤è€…é‚®ç®±
                                    "committed_datetime": commit.committed_datetime.isoformat(),
                                    "message": commit.message.strip(),
                                    "insertions": stats.get("insertions", 0),
                                    "deletions": stats.get("deletions", 0),
                                    "files_changed": stats.get("files", 0),
                                }
                                stats = commit.stats.files
                                for file_path, file_stat in stats.items():
                                    # è·³è¿‡å•æ–‡ä»¶æ’å…¥è¡Œæ•°è¿‡å¤§çš„æ–‡ä»¶ï¼ˆå¦‚åº“æ–‡ä»¶/ä¾èµ–/å¤§æ–‡ä»¶ï¼‰
                                    if (
                                        file_stat.get("insertions", 0)
                                        > self.single_file_insertion_limit
                                    ):
                                        continue
                                    # åˆ çš„å¤ªå¤šä¹Ÿä¸è¡Œ
                                    if (
                                        file_stat.get("deletions", 0)
                                        > self.single_file_insertion_limit
                                    ):
                                        continue
                                    _, ext = os.path.splitext(file_path)
                                    ext = ext.lower()
                                    if ext in self.valid_extensions:
                                        commit_file_stats[ext] += max(
                                            file_stat["insertions"]
                                            - file_stat["deletions"],
                                            0,
                                        )  # åˆ«æ•´æˆè´Ÿæ•°äº†
                                commit_info_list.append(commit_info)
                    self._log(f"å¤„ç†ä»“åº“ {repo_path} å®Œæˆï¼Œå…± {len(commit_info_list)} ä¸ªcommit")
                    return {
                        "repo_name": repo_name,
                        "group_name": group_name,
                        "commits": commit_info_list,
                        "code_line_data": {
                            "total_lines": sum(commit_file_stats.values()),
                            "ext_status": commit_file_stats,
                        },
                    }
                except Exception as e:
                    self._log(f"å¤„ç†ä»“åº“ {repo_path} å¤±è´¥: {e}")
                    return None

            if self.MULTI_THREAD:
                # å¤šçº¿ç¨‹åŠ é€Ÿå¤„ç†
                try:
                    with ThreadPoolExecutor(max_workers=self.WORKERS) as executor:
                        future_to_repo = {
                            executor.submit(process_repo, path): path
                            for path in repo_paths
                        }
                        for future in as_completed(future_to_repo):
                            repo_path = future_to_repo[future]
                            try:
                                result = future.result(timeout=60)
                                if result:
                                    results.append(result)
                            except Exception as e:
                                self._log(f"å¤„ç†ä»“åº“ {repo_path} å¤±è´¥: {e}")
                except KeyboardInterrupt:
                    self._log("æ£€æµ‹åˆ°ä¸­æ–­ï¼Œæ­£åœ¨é€€å‡ºçº¿ç¨‹æ± ...")
                    executor.shutdown(wait=False, cancel_futures=True)
                    raise
            else:
                for repo_path in repo_paths:
                    result = process_repo(repo_path)
                    if result:
                        results.append(result)
            all_results[semestar_name] = results
            self._log(f"å­¦æœŸ {semestar_name} å¤„ç†å®Œæˆï¼Œå…±{len(results)}ä¸ªä»“åº“ã€‚")
        self.tmp_data["local_data"] = all_results
        self._save_json(self.tmp_path, self.tmp_data)
        self._log("æœ¬åœ°ä»“åº“æ•°æ®æ”¶é›†å®Œæˆã€‚")

    # ========== 3. è¿œç¨‹PRçˆ¬å– ===========
    def fetch_prs(self):
        """
        å¹¶å‘çˆ¬å–æ‰€æœ‰ä»“åº“çš„PRä¿¡æ¯ï¼Œå­˜å…¥tmp_data['pr']
        """
        PR_QUERY = """
        query($owner: String!, $repo: String!, $first: Int!, $after: String) {
          repository(owner: $owner, name: $repo) {
            pullRequests(first: $first, after: $after, orderBy: {field: CREATED_AT, direction: ASC}) {
              pageInfo {
                hasNextPage
                endCursor
              }
              nodes {
                title
                createdAt
                closedAt
                mergedAt
                number
                author {
                  login
                }
                commits {
                  totalCount
                }
              }
            }
          }
        }
        """
        self._log("å¼€å§‹çˆ¬å–æ‰€æœ‰ä»“åº“çš„PRä¿¡æ¯...")
        pr_data = {}
        # è·å–æ‰€æœ‰repoå
        all_repos = []
        for semestar, repos in self.tmp_data.get("local_data", {}).items():
            for repo in repos:
                all_repos.append(repo["repo_name"])

        def fetch_one_repo_pr(repo_name):
            all_prs = []
            has_next = True
            cursor = None
            while has_next:
                variables = {
                    "owner": self.organization,
                    "repo": repo_name,
                    "first": 50,
                    "after": cursor,
                }
                try:
                    result = self._run_query(PR_QUERY, variables)
                    repo_data = result["data"]["repository"]["pullRequests"]
                    all_prs.extend(repo_data["nodes"])
                    has_next = repo_data["pageInfo"]["hasNextPage"]
                    cursor = repo_data["pageInfo"]["endCursor"]
                except Exception as e:
                    self._log(f"çˆ¬å–PRå¤±è´¥: {repo_name}: {e}")
                    break
            self._log(f"{repo_name} PRæ•°: {len(all_prs)}")
            return repo_name, all_prs

        if self.MULTI_THREAD:
            try:
                with ThreadPoolExecutor(max_workers=self.WORKERS) as executor:
                    futures = {
                        executor.submit(fetch_one_repo_pr, repo): repo
                        for repo in all_repos
                    }
                    for future in as_completed(futures):
                        repo = futures[future]
                        try:
                            repo, prs = future.result(timeout=30)
                            pr_data[repo] = prs
                        except Exception as e:
                            self._log(f"PRä»»åŠ¡å¼‚å¸¸: {repo} - {e}")
            except KeyboardInterrupt:
                self._log("æ£€æµ‹åˆ°ä¸­æ–­ï¼Œæ­£åœ¨é€€å‡ºçº¿ç¨‹æ± ...")
                executor.shutdown(wait=False, cancel_futures=True)
                raise
        else:
            for repo in all_repos:
                repo, prs = fetch_one_repo_pr(repo)
                pr_data[repo] = prs
        self.tmp_data["pr"] = pr_data
        self._save_json(self.tmp_path, self.tmp_data)
        self._log("æ‰€æœ‰PRä¿¡æ¯çˆ¬å–å®Œæˆã€‚")

    # ========== 3.2 è¿œç¨‹Issueçˆ¬å– ===========
    def fetch_issues(self):
        """
        å¹¶å‘çˆ¬å–æ‰€æœ‰ä»“åº“çš„Issueä¿¡æ¯ï¼Œå­˜å…¥tmp_data['issues']
        """
        ISSUE_QUERY = """
        query($owner: String!, $repo: String!, $first: Int!, $after: String) {
          repository(owner: $owner, name: $repo) {
            issues(first: $first, after: $after, orderBy: {field: CREATED_AT, direction: ASC}) {
              pageInfo {
                hasNextPage
                endCursor
              }
              nodes {
                title
                createdAt
                closedAt
                number
                author {
                  login
                }
                comments {
                  totalCount
                }
                labels(first: 5) {
                  nodes {
                    name
                  }
                }
              }
            }
          }
        }
        """
        self._log("å¼€å§‹çˆ¬å–æ‰€æœ‰ä»“åº“çš„Issueä¿¡æ¯...")
        issue_data = {}
        all_repos = []
        for semestar, repos in self.tmp_data.get("local_data", {}).items():
            for repo in repos:
                all_repos.append(repo["repo_name"])

        def fetch_one_repo_issue(repo_name):
            all_issues = []
            has_next = True
            cursor = None
            while has_next:
                variables = {
                    "owner": self.organization,
                    "repo": repo_name,
                    "first": 50,
                    "after": cursor,
                }
                try:
                    result = self._run_query(ISSUE_QUERY, variables)
                    repo_data = result["data"]["repository"]["issues"]
                    all_issues.extend(repo_data["nodes"])
                    has_next = repo_data["pageInfo"]["hasNextPage"]
                    cursor = repo_data["pageInfo"]["endCursor"]
                except Exception as e:
                    self._log(f"çˆ¬å–Issueå¤±è´¥: {repo_name}: {e}")
                    break
            self._log(f"{repo_name} Issueæ•°: {len(all_issues)}")
            return repo_name, all_issues

        if self.MULTI_THREAD:
            try:
                with ThreadPoolExecutor(max_workers=self.WORKERS) as executor:
                    futures = {
                        executor.submit(fetch_one_repo_issue, repo): repo
                        for repo in all_repos
                    }
                    for future in as_completed(futures):
                        repo = futures[future]
                        try:
                            repo, issues = future.result(timeout=30)
                            issue_data[repo] = issues
                        except Exception as e:
                            self._log(f"Issueä»»åŠ¡å¼‚å¸¸: {repo} - {e}")
            except KeyboardInterrupt:
                self._log("æ£€æµ‹åˆ°ä¸­æ–­ï¼Œæ­£åœ¨é€€å‡ºçº¿ç¨‹æ± ...")
                executor.shutdown(wait=False, cancel_futures=True)
                raise
        else:
            for repo in all_repos:
                repo, issues = fetch_one_repo_issue(repo)
                issue_data[repo] = issues
        self.tmp_data["issues"] = issue_data
        self._save_json(self.tmp_path, self.tmp_data)
        self._log("æ‰€æœ‰Issueä¿¡æ¯çˆ¬å–å®Œæˆã€‚")

    # ========== 3.3 è¿œç¨‹Branchçˆ¬å– ===========
    def fetch_branches(self):
        """
        å¹¶å‘çˆ¬å–æ‰€æœ‰ä»“åº“çš„åˆ†æ”¯ä¿¡æ¯ï¼Œå­˜å…¥tmp_data['branches']
        """
        BRANCH_QUERY = """
        query($owner: String!, $repo: String!, $first: Int!, $after: String) {
          repository(owner: $owner, name: $repo) {
            refs(refPrefix: \"refs/heads/\", first: $first, after: $after) {
              pageInfo {
                hasNextPage
                endCursor
              }
              nodes {
                name
                target {
                  ... on Commit {
                    committedDate
                  }
                }
              }
            }
          }
        }
        """
        self._log("å¼€å§‹çˆ¬å–æ‰€æœ‰ä»“åº“çš„åˆ†æ”¯ä¿¡æ¯...")
        branch_data = {}
        all_repos = []
        for semestar, repos in self.tmp_data.get("local_data", {}).items():
            for repo in repos:
                all_repos.append(repo["repo_name"])

        def fetch_one_repo_branch(repo_name):
            all_branches = []
            has_next = True
            cursor = None
            while has_next:
                variables = {
                    "owner": self.organization,
                    "repo": repo_name,
                    "first": 50,
                    "after": cursor,
                }
                try:
                    result = self._run_query(BRANCH_QUERY, variables)
                    refs_data = result["data"]["repository"]["refs"]
                    all_branches.extend(refs_data["nodes"])
                    has_next = refs_data["pageInfo"]["hasNextPage"]
                    cursor = refs_data["pageInfo"]["endCursor"]
                except Exception as e:
                    self._log(f"çˆ¬å–Branchå¤±è´¥: {repo_name}: {e}")
                    break
            self._log(f"{repo_name} åˆ†æ”¯æ•°: {len(all_branches)}")
            return repo_name, all_branches

        if self.MULTI_THREAD:
            try:
                with ThreadPoolExecutor(max_workers=self.WORKERS) as executor:
                    futures = {
                        executor.submit(fetch_one_repo_branch, repo): repo
                        for repo in all_repos
                    }
                    for future in as_completed(futures):
                        repo = futures[future]
                        try:
                            repo, branches = future.result(timeout=30)
                            branch_data[repo] = branches
                        except Exception as e:
                            self._log(f"Branchä»»åŠ¡å¼‚å¸¸: {repo} - {e}")
            except KeyboardInterrupt:
                self._log("æ£€æµ‹åˆ°ä¸­æ–­ï¼Œæ­£åœ¨é€€å‡ºçº¿ç¨‹æ± ...")
                executor.shutdown(wait=False, cancel_futures=True)
                raise
        else:
            for repo in all_repos:
                repo, branches = fetch_one_repo_branch(repo)
                branch_data[repo] = branches
        self.tmp_data["branches"] = branch_data
        self._save_json(self.tmp_path, self.tmp_data)
        self._log("æ‰€æœ‰åˆ†æ”¯ä¿¡æ¯çˆ¬å–å®Œæˆã€‚")

    # ========== 3.4 è¿œç¨‹æäº¤ä½œè€…ä¿¡æ¯çˆ¬å– ===========
    def fetch_commit_authors(self):
        """
        å¹¶å‘çˆ¬å–æ‰€æœ‰ä»“åº“çš„æäº¤ä½œè€…ä¿¡æ¯ï¼Œå­˜å…¥tmp_data['commit_authors']
        """
        COMMIT_USER_QUERY = """
        query($owner: String!, $name: String!, $expression: String!) {
          repository(owner: $owner, name: $name) {
            object(expression: $expression) {
              ... on Commit {
                author {
                  user {
                    id
                  }
                  name
                  email
                }
              }
            }
          }
        }
        """
        self._log("å¼€å§‹çˆ¬å–æ‰€æœ‰ä»“åº“çš„æäº¤ä½œè€…ä¿¡æ¯...")
        author_data = {}

        # ä»æœ¬åœ°æ•°æ®ä¸­è·å–éœ€è¦æŸ¥è¯¢çš„æäº¤
        commits_to_fetch = {}  # repo -> {commits_author: commithash, }
        for semestar, repos in self.tmp_data.get("local_data", {}).items():
            for repo in repos:
                email2hash = {}
                commit_to_fetch_in_repo = []
                for commit in repo["commits"]:
                    if commit["author_email"] not in email2hash:
                        email2hash[commit["author_email"]] = commit["commit_hash"]
                        commit_to_fetch_in_repo.append(commit["commit_hash"])
                commits_to_fetch[repo["repo_name"]] = commit_to_fetch_in_repo

        def fetch_commit_author(repo_name, commit_sha):
            variables = {
                "owner": self.organization,
                "name": repo_name,
                "expression": commit_sha,
            }
            try:
                result = self._run_query(COMMIT_USER_QUERY, variables)
                author_info = result["data"]["repository"]["object"]["author"]
                return {
                    "commit": commit_sha,
                    "id": author_info["user"]["id"] if author_info["user"] else None,
                    "name": author_info["name"],
                    "email": author_info["email"],
                }
            except Exception as e:
                self._log(f"çˆ¬å–æäº¤ä½œè€…ä¿¡æ¯å¤±è´¥: {repo_name}/{commit_sha}: {e}")
                return {"commit": commit_sha, "id": None, "name": None, "email": None}

        def process_repo(repo_name, commit_shas):
            self._log(
                f"å¼€å§‹çˆ¬å– {repo_name} çš„æäº¤ä½œè€…ä¿¡æ¯ - {len(commit_shas)} ä¸ªæäº¤"
            )
            users = []
            if self.MULTI_THREAD:
                with ThreadPoolExecutor(max_workers=self.WORKERS) as executor:
                    futures = {
                        executor.submit(fetch_commit_author, repo_name, sha): sha
                        for sha in commit_shas
                    }
                    for future in as_completed(futures):
                        result = future.result()
                        if result:
                            users.append(result)
            else:
                for sha in commit_shas:
                    result = fetch_commit_author(repo_name, sha)
                    if result:
                        users.append(result)
            return repo_name, users

        if self.MULTI_THREAD:
            try:
                with ThreadPoolExecutor(max_workers=self.WORKERS) as executor:
                    futures = {
                        executor.submit(process_repo, repo, commits): repo
                        for repo, commits in commits_to_fetch.items()
                    }
                    for future in as_completed(futures):
                        repo = futures[future]
                        try:
                            repo, users = future.result(timeout=60)
                            author_data[repo] = users
                        except Exception as e:
                            self._log(f"å¤„ç†ä»“åº“æäº¤ä½œè€…ä¿¡æ¯ä»»åŠ¡å¼‚å¸¸: {repo} - {e}")
            except KeyboardInterrupt:
                self._log("æ£€æµ‹åˆ°ä¸­æ–­ï¼Œæ­£åœ¨é€€å‡ºçº¿ç¨‹æ± ...")
                executor.shutdown(wait=False, cancel_futures=True)
                raise
        else:
            for repo, commits in commits_to_fetch.items():
                repo, users = process_repo(repo, commits)
                author_data[repo] = users

        self.tmp_data["commit_authors"] = author_data
        self._save_json(self.tmp_path, self.tmp_data)
        self._log("æ‰€æœ‰æäº¤ä½œè€…ä¿¡æ¯çˆ¬å–å®Œæˆã€‚")

    # ========== 4 è¿‡æ»¤æäº¤ä¿¡æ¯ ===========
    def filter_commits_by_classroom_user(self):
        self._log("å¼€å§‹è¿‡æ»¤æäº¤ä¿¡æ¯...")
        self.tmp_data["filtered_local_data"] = {}
        for semestar, repos in self.tmp_data.get("local_data", {}).items():
            self.tmp_data["filtered_local_data"][semestar] = []
            for repo in repos:
                remote_commitor_info = self.tmp_data["commit_authors"].get(
                    repo["repo_name"], []
                )
                email2id = {c["email"]: c["id"] for c in remote_commitor_info}
                group_members = (
                    self.tmp_data["group_members"]
                    .get(semestar, {})
                    .get(repo["repo_name"], [])
                )
                valid_ids = set(p["github_id"] for p in group_members)
                valid_commits = []
                for commit in repo["commits"]:
                    if commit["author_email"] not in email2id:  # é‰´å®šä¸ºå¤–éƒ¨commitor
                        continue
                    if (
                        email2id[commit["author_email"]] in valid_ids
                    ):  # é‰´å®šä¸ºå°ç»„å†…æˆå‘˜
                        commit["author_id"] = email2id[commit["author_email"]]
                        valid_commits.append(commit)
                if len(valid_commits) == 0:
                    continue

                self.tmp_data["filtered_local_data"][semestar].append(
                    {
                        "repo_name": repo["repo_name"],
                        "group_name": repo["group_name"],
                        "commits": valid_commits,
                        "code_line_data": repo["code_line_data"],
                        "group_member_ids": [
                            val for val in valid_ids if val is not None
                        ],
                    }
                )
        self._save_json(self.tmp_path, self.tmp_data)
        self._log("æ‰€æœ‰æäº¤ä¿¡æ¯è¿‡æ»¤å®Œæˆã€‚")

    # ========== 5. å›¾è¡¨æ•°æ®ç”Ÿæˆ ===========
    def generate_chart_data(self):
        """
        ç”Ÿæˆæ‰€æœ‰å›¾è¡¨æ‰€éœ€æ•°æ®ï¼Œå­˜å…¥chart_data.json
        """
        self._log("å¼€å§‹ç”Ÿæˆå›¾è¡¨æ•°æ®...")
        chart_data = {}
        # ä»¥å­¦æœŸä¸ºå•ä½
        for semestar, repos in self.tmp_data.get("filtered_local_data", {}).items():
            # 1. commitæ—¶é—´åˆ†å¸ƒï¼ˆæ—¥ï¼‰
            all_commits = []
            [all_commits.extend(repo["commits"]) for repo in repos]
            date_list = []
            for commit in all_commits:
                dt = datetime.fromisoformat(commit["committed_datetime"])
                dt_china = dt.astimezone(self.china_tz)
                date_str = dt_china.date().isoformat()
                date_list.append(date_str)
            commit_counter = Counter(date_list)
            if date_list:
                start_date = datetime.fromisoformat(min(date_list))
                end_date = datetime.fromisoformat(max(date_list))
                full_dates = []
                counts = []
                current = start_date
                while current <= end_date:
                    date_str = current.date().isoformat()
                    full_dates.append(date_str)
                    counts.append(commit_counter.get(date_str, 0))
                    current += timedelta(days=1)
            else:
                full_dates, counts = [], []
            chart_data.setdefault(semestar, {})["commit_time_distribution_date"] = {
                "full_dates": full_dates,
                "counts": counts,
            }
            # 2. commitæ—¶é—´åˆ†å¸ƒï¼ˆå°æ—¶ï¼‰
            hour_list = []
            for commit in all_commits:
                dt = datetime.fromisoformat(commit["committed_datetime"])
                dt_china = dt.astimezone(self.china_tz)
                hour = dt_china.hour
                hour_list.append(hour)
            commit_counter_hour = Counter(hour_list)
            full_hours = list(range(24))
            hour_counts = [commit_counter_hour.get(h, 0) for h in full_hours]
            chart_data[semestar]["commit_time_distribution_hourly"] = {
                "hours": full_hours,
                "counts": hour_counts,
            }
            # 3. æ¯ç»„commitæ•°
            group_names = [repo["group_name"] for repo in repos]
            commit_counts = [len(repo["commits"]) for repo in repos]
            avg_commit_count = (
                round(sum(commit_counts) / len(commit_counts), 2)
                if commit_counts
                else 0
            )
            chart_data[semestar]["commit_count_per_repo"] = {
                "group_names": group_names,
                "commit_counts": commit_counts,
                "average_commit_count": avg_commit_count,
            }
            # 4. ä»£ç è¡Œæ•°åˆ†å¸ƒ
            code_line_counts = [
                repo.get("code_line_data", {}).get("total_lines", 0) for repo in repos
            ]
            avg_lines = (
                sum(code_line_counts) / len(code_line_counts) if code_line_counts else 0
            )
            chart_data[semestar]["code_line_per_repo"] = {
                "group_names": group_names,
                "total_lines": code_line_counts,
                "average_lines": avg_lines,
            }
            # 5. è¯­è¨€åˆ†å¸ƒ
            language_counter = Counter()
            for repo in repos:
                code_data = repo.get("code_line_data", {})
                lang_data = code_data.get("ext_status", {})
                for lang, count in lang_data.items():
                    language_counter[lang] += count
            sorted_lang = sorted(
                language_counter.items(), key=lambda x: x[1], reverse=True
            )
            languages = [lang for lang, _ in sorted_lang]
            lang_counts = [count for _, count in sorted_lang]
            chart_data[semestar]["language_distribution"] = {
                "languages": languages,
                "counts": lang_counts,
            }
            # 6. PRæ•°/Issueæ•°/åˆ†æ”¯æ•°
            pr_counts = [
                len(self.tmp_data.get("pr", {}).get(repo["repo_name"], []))
                for repo in repos
            ]
            issue_counts = [
                len(self.tmp_data.get("issues", {}).get(repo["repo_name"], []))
                for repo in repos
            ]
            branch_counts = [
                len(self.tmp_data.get("branches", {}).get(repo["repo_name"], []))
                for repo in repos
            ]
            chart_data[semestar]["pr_count_per_repo"] = {
                "group_names": group_names,
                "pr_counts": pr_counts,
                "average_pr": sum(pr_counts) / len(pr_counts) if pr_counts else 0,
            }
            chart_data[semestar]["issue_count_per_repo"] = {
                "group_names": group_names,
                "issue_counts": issue_counts,
                "average_issues": (
                    sum(issue_counts) / len(issue_counts) if issue_counts else 0
                ),
            }
            chart_data[semestar]["branch_count_per_repo"] = {
                "group_names": group_names,
                "branch_counts": branch_counts,
                "average_branches": (
                    sum(branch_counts) / len(branch_counts) if branch_counts else 0
                ),
            }
            # 7. PRçŠ¶æ€åˆ†å¸ƒ
            pr_status_counter = {"merged": 0, "closed": 0, "open": 0}
            for repo in repos:
                repo_prs = self.tmp_data.get("pr", {}).get(repo["repo_name"], [])
                for pr in repo_prs:
                    if pr.get("mergedAt"):
                        pr_status_counter["merged"] += 1
                    elif pr.get("closedAt"):
                        pr_status_counter["closed"] += 1
                    else:
                        pr_status_counter["open"] += 1
            chart_data[semestar]["pr_status_distribution"] = pr_status_counter
            # 8. IssueçŠ¶æ€åˆ†å¸ƒ
            issue_status_counter = {"open": 0, "closed": 0}
            for repo in repos:
                repo_issues = self.tmp_data.get("issues", {}).get(repo["repo_name"], [])
                for issue in repo_issues:
                    if issue.get("closedAt"):
                        issue_status_counter["closed"] += 1
                    else:
                        issue_status_counter["open"] += 1
            chart_data[semestar]["issue_status_distribution"] = issue_status_counter
            # 9. å°ç»„æˆå‘˜æ•°é‡åˆ†å¸ƒ
            repo_active_counts = []
            active_count_bucket = Counter()
            for repo in repos:
                active_count = len(repo["group_member_ids"])
                repo_active_counts.append(
                    {
                        "group_name": repo["group_name"],
                        "active_contributor_count": active_count,
                    }
                )
                active_count_bucket[active_count] += 1
            pie_chart_data = {str(k): v for k, v in sorted(active_count_bucket.items())}
            chart_data[semestar]["repo_active_contributor_count"] = repo_active_counts
            chart_data[semestar]["active_contributor_pie_chart"] = pie_chart_data

            # 10. è´¡çŒ®å·®å¼‚ï¼ˆGiniç³»æ•°ï¼‰
            def gini_coefficient(values):
                values = sorted(values)
                n = len(values)
                if n == 0:
                    return 0
                total = sum(values)
                if total == 0:
                    return 0
                cumulative = 0
                for i, value in enumerate(values):
                    cumulative += (i + 1) * value
                return (2 * cumulative) / (n * total) - (n + 1) / n

            group_names_gini = []
            group_gini_commit = []
            group_gini_change_lines = []
            for repo in repos:
                commit_count_per_people = {id: 0 for id in repo["group_member_ids"]}
                change_lines_per_people = {id: 0 for id in repo["group_member_ids"]}
                for commit in repo["commits"]:
                    author_id = commit.get("author_id")
                    if author_id not in commit_count_per_people:
                        commit_count_per_people[author_id] = 0
                        change_lines_per_people[author_id] = 0
                    commit_count_per_people[author_id] += 1
                    change_lines_per_people[author_id] += commit.get(
                        "insertions", 0
                    ) + commit.get("deletions", 0)
                commit_counts = list(commit_count_per_people.values())
                add_lines = list(change_lines_per_people.values())
                gini_commit = gini_coefficient(commit_counts)
                gini_add_lines = gini_coefficient(add_lines)
                group_names_gini.append(repo["group_name"])
                group_gini_commit.append(gini_commit)
                group_gini_change_lines.append(gini_add_lines)
            chart_data[semestar]["contribution_difference"] = {
                "group_names": group_names_gini,
                "gini_commit": group_gini_commit,
                "gini_add_lines": group_gini_change_lines,
            }

            # 11. commit messageä¿¡æ¯ç»Ÿè®¡
            def classify_message(message):
                chinese_chars = re.findall(r"[\u4e00-\u9fff]", message)
                english_words = re.findall(r"\b[a-zA-Z]+\b", message)
                chinese_count = len(chinese_chars)
                english_count = len(english_words)
                total_count = chinese_count + english_count
                if total_count == 0:
                    return "mixed", 0
                if (
                    chinese_count >= 0.4 * total_count
                    and english_count >= 0.4 * total_count
                ):
                    lang = "mixed"
                elif chinese_count > english_count:
                    lang = "chinese"
                elif english_count > chinese_count:
                    lang = "english"
                else:
                    lang = "mixed"
                return lang, total_count

            def get_length_distribution(lengths, bin_width=5):
                if not lengths:
                    return []
                min_len = min(lengths)
                max_len = max(lengths)
                num_bins = math.ceil((max_len - min_len + 1) / bin_width)
                bins = [0] * num_bins
                for length in lengths:
                    index = min((length - min_len) // bin_width, num_bins - 1)
                    bins[index] += 1
                distribution = []
                for i, count in enumerate(bins):
                    range_start = min_len + i * bin_width
                    range_end = range_start + bin_width - 1
                    distribution.append(
                        {"length_range": f"{range_start}â€“{range_end}", "count": count}
                    )
                return distribution

            lang_counter = {"chinese": 0, "english": 0, "mixed": 0}
            lengths = []
            for repo in repos:
                for commit in repo["commits"]:
                    lang, length = classify_message(commit.get("message", ""))
                    if lang in lang_counter:
                        lang_counter[lang] += 1
                    if length <= 100:
                        lengths.append(length)
            distribution = get_length_distribution(lengths)
            chart_data[semestar]["commit_message_info"] = {
                "lang_counter": lang_counter,
                "length_distribution": distribution,
            }
        self.chart_data = chart_data
        self._save_json(self.chart_data_path, self.chart_data)
        self._log("å›¾è¡¨æ•°æ®ç”Ÿæˆå®Œæˆã€‚")

    # ========== 6. ä¿å­˜æ‰€æœ‰æ•°æ® ===========
    def save_all(self):
        self._save_json(self.tmp_path, self.tmp_data)
        self._save_json(self.chart_data_path, self.chart_data)
        self._log("æ‰€æœ‰æ•°æ®å·²ä¿å­˜ã€‚")

    def auto_run(self):
        """
        ä¸€é”®è‡ªåŠ¨å®Œæˆæ‰€æœ‰æ•°æ®çˆ¬å–ã€è¿‡æ»¤ã€å›¾è¡¨ç”Ÿæˆå’Œä¿å­˜ã€‚
        æ­£ç¡®çš„è°ƒç”¨é¡ºåºï¼š
        1. è·å– GitHub Classroom æˆå‘˜ä¿¡æ¯
        2. è·å–ä½œä¸šçš„å°ç»„æˆå‘˜ä¿¡æ¯
        3. å…³è”å°ç»„æˆå‘˜ä¸ classroom æˆå‘˜
        4. æ”¶é›†æœ¬åœ°ä»“åº“æ•°æ®
        5. è·å–è¿œç¨‹æäº¤ä½œè€…ä¿¡æ¯
        6. è¿‡æ»¤æäº¤ä¿¡æ¯
        7. è·å–è¿œç¨‹ PR/Issue/åˆ†æ”¯ä¿¡æ¯
        8. ç”Ÿæˆå›¾è¡¨æ•°æ®
        9. ä¿å­˜æ‰€æœ‰æ•°æ®
        """
        self._log("======= ä¸€é”®è‡ªåŠ¨å¼€å§‹å…¨æµç¨‹ =======")
        
        # 1. è·å– GitHub Classroom æˆå‘˜ä¿¡æ¯
        self._log("æ­¥éª¤ 1: è·å– GitHub Classroom æˆå‘˜ä¿¡æ¯")
        # self.get_classroom_members()
        
        # 2. è·å–ä½œä¸šçš„å°ç»„æˆå‘˜ä¿¡æ¯
        self._log("æ­¥éª¤ 2: è·å–ä½œä¸šçš„å°ç»„æˆå‘˜ä¿¡æ¯")
        # self.get_accepted_assignments()
        
        # 3. å…³è”å°ç»„æˆå‘˜ä¸ classroom æˆå‘˜
        self._log("æ­¥éª¤ 3: å…³è”å°ç»„æˆå‘˜ä¸ classroom æˆå‘˜")
        # self.join_classroom_members_with_group_members()
        
        # 4. æ”¶é›†æœ¬åœ°ä»“åº“æ•°æ®
        self._log("æ­¥éª¤ 4: æ”¶é›†æœ¬åœ°ä»“åº“æ•°æ®")
        # self.gather_data_from_local_repos()
        
        # 5. è·å–è¿œç¨‹æäº¤ä½œè€…ä¿¡æ¯
        self._log("æ­¥éª¤ 5: è·å–è¿œç¨‹æäº¤ä½œè€…ä¿¡æ¯")
        self.fetch_commit_authors()
        
        # 6. è¿‡æ»¤æäº¤ä¿¡æ¯
        self._log("æ­¥éª¤ 6: è¿‡æ»¤æäº¤ä¿¡æ¯")
        self.filter_commits_by_classroom_user()
        
        # 7. è·å–è¿œç¨‹ PR/Issue/åˆ†æ”¯ä¿¡æ¯
        self._log("æ­¥éª¤ 7: è·å–è¿œç¨‹ PR ä¿¡æ¯")
        self.fetch_prs()
        
        self._log("æ­¥éª¤ 8: è·å–è¿œç¨‹ Issue ä¿¡æ¯")
        self.fetch_issues()
        
        self._log("æ­¥éª¤ 9: è·å–è¿œç¨‹åˆ†æ”¯ä¿¡æ¯")
        self.fetch_branches()
        
        # 8. ç”Ÿæˆå›¾è¡¨æ•°æ®
        self._log("æ­¥éª¤ 10: ç”Ÿæˆå›¾è¡¨æ•°æ®")
        self.generate_chart_data()
        
        # 9. ä¿å­˜æ‰€æœ‰æ•°æ®
        self._log("æ­¥éª¤ 11: ä¿å­˜æ‰€æœ‰æ•°æ®")
        self.save_all()
        
        self._log("======= å…¨æµç¨‹è‡ªåŠ¨å®Œæˆ =======")


# ========== ç”¨æ³•ç¤ºä¾‹ ===========
if __name__ == "__main__":
    spider = GithubClassroomSpider()
    spider.auto_run()
    # spider.get_project_assignment_id()
    # spider.get_accepted_assignments()
    # spider.join_classroom_members_with_group_members()
    # spider.filter_commits_by_classroom_user()
    # spider.gather_data_from_local_repos()
    # spider.filter_commits_by_classroom_user()
    # spider.generate_chart_data()
